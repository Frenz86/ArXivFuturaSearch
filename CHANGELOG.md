# Changelog

All notable changes to ArXiv RAG Copilot are documented in this file.

## [0.3.0] - 2026-01-31

### üéØ Major Refactoring: Complete LangChain Integration + ChromaDB Only

This release represents a complete architectural refactoring to use the LangChain framework and simplifies the architecture by removing FAISS support in favor of ChromaDB-only deployment.

### üöÄ New Features

#### LangChain Components
- **Embeddings**: Migrated to `langchain_huggingface.HuggingFaceEmbeddings`
  - Uses LangChain's embedding interface internally
  - Maintains backward-compatible wrapper API
  - New method: `get_langchain_embeddings()` for direct access

- **Vector Stores**: Full LangChain integration
  - ChromaDB: Uses `langchain_chroma.Chroma` with persistent client
  - FAISS: Uses `langchain_community.vectorstores.FAISS`
  - Both support LangChain retriever interface
  - New methods: `get_langchain_retriever()`, `get_ensemble_retriever()`

- **Retrievers**: LangChain-native retrievers
  - `EnsembleRetriever` for hybrid search (semantic + BM25)
  - `BM25Retriever` for lexical search
  - Support for MMR (Maximal Marginal Relevance) via LangChain
  - Filter support for metadata-based retrieval

- **LLMs**: Unified LangChain LLM interface
  - `ChatOpenAI` for OpenRouter (with custom base_url)
  - `Ollama` for local models
  - `FakeListLLM` for mock mode
  - Function: `get_llm()` provides configured LLM instance

- **Prompts**: LangChain PromptTemplates
  - `COT_PROMPT_TEMPLATE` for chain-of-thought prompting
  - `SIMPLE_PROMPT_TEMPLATE` for direct answering
  - Easily customizable and composable

- **Chains**: Native LangChain LCEL chains
  - New function: `create_rag_chain()` - Complete RAG pipeline
  - Composable with `|` operator (LCEL syntax)
  - Support for streaming and async execution

- **Document Loaders**: Custom ArXiv loader
  - New file: `app/arxiv_document_loader.py`
  - `ArXivLoader` - Loads papers with optional chunking
  - `ArXivPaperLoader` - Convenience loader for single papers
  - Integrates with semantic/sentence-aware chunking

### üì¶ Dependencies

#### Added
- `langchain>=0.3.0` - Core framework
- `langchain-chroma>=0.2.0` - ChromaDB integration
- `langchain-community>=0.3.0` - Community integrations (BM25, FAISS, Ollama)
- `langchain-huggingface>=0.1.0` - HuggingFace embeddings
- `langchain-openai>=0.2.0` - OpenAI/OpenRouter LLM support

#### Updated
- Version bumped to `0.3.0` in `pyproject.toml`

### üîÑ Modified Files

#### Core Components
- `app/embeddings.py` - Refactored to use `HuggingFaceEmbeddings`
  - Maintains singleton pattern
  - Added `get_langchain_embeddings()` method
  - Backward compatible `embed()` and `embed_query()` methods

- `app/vectorstore_chroma.py` - Complete LangChain integration
  - Uses `langchain_chroma.Chroma` internally
  - Uses `EnsembleRetriever` for hybrid search
  - New methods for LangChain retriever access
  - Maintains backward-compatible `search()` API

- `app/vectorstore.py` - FAISS with LangChain
  - Uses `langchain_community.vectorstores.FAISS`
  - Uses `EnsembleRetriever` for hybrid search
  - Enhanced save/load with `save_local()`/`load_local()`
  - Maintains backward compatibility

- `app/rag.py` - Complete refactoring
  - Prompt templates as LangChain `PromptTemplate` objects
  - LLM access via `get_llm()`
  - New `create_rag_chain()` for composable chains
  - Streaming via LangChain's `astream()`
  - Renamed `check_openrouter_health()` to `check_llm_health()`
  - Maintains backward-compatible function signatures

- `app/main.py` - Updated imports and version
  - Version updated to v0.3.0
  - Imports `check_llm_health` instead of `check_openrouter_health`
  - Added `create_rag_chain` import for future use
  - All endpoints work unchanged due to backward compatibility

#### New Files
- `app/arxiv_document_loader.py` - LangChain document loaders
  - `ArXivLoader(BaseLoader)` - Batch and lazy loading
  - `ArXivPaperLoader(BaseLoader)` - Single paper loader
  - Integrates with chunking strategies

#### Documentation
- `README.md` - Major update
  - New v0.3.0 features section
  - "Using LangChain Components" section with examples:
    - Document loaders
    - Retrievers (semantic, MMR, ensemble)
    - RAG chains
    - Direct LLM access
    - Custom chain composition
  - Code examples for programmatic usage

- `CHANGELOG.md` - This entry

### üéØ Benefits

#### For Developers
- **Composability**: Build custom chains using LCEL syntax
- **Ecosystem**: Access to entire LangChain ecosystem (agents, tools, memory)
- **Best Practices**: Use battle-tested LangChain patterns
- **Documentation**: Leverage extensive LangChain documentation
- **Future-Proof**: Easy integration of new LangChain features

#### For Users
- **Backward Compatibility**: Existing API unchanged
- **No Breaking Changes**: All v0.2.0 code continues to work
- **Performance**: Same or better performance
- **Flexibility**: Can use either FastAPI endpoints OR LangChain chains directly

### üîß Migration Guide

#### If You're Using the API (FastAPI Endpoints)
**No changes required!** All endpoints work exactly as before.

#### If You're Using the Code Directly
The refactored components maintain the same API surface. However, you can now also use LangChain components directly:

**Before (still works):**
```python
from app.vectorstore_chroma import ChromaHybridStore

store = ChromaHybridStore()
results = store.search(query_vec, query_text="example", top_k=5)
```

**After (new LangChain way):**
```python
from app.vectorstore_chroma import ChromaHybridStore

store = ChromaHybridStore()
retriever = store.get_langchain_retriever(search_type="mmr")
docs = retriever.invoke("example")
```

**Before (still works):**
```python
from app.rag import llm_generate_async

answer = await llm_generate_async(prompt)
```

**After (new LangChain way):**
```python
from app.rag import get_llm

llm = get_llm()
answer = await llm.ainvoke(prompt)
```

### üóëÔ∏è Removed Features (Simplification)

#### FAISS Vector Store Removed
- **Removed**: FAISS vector store implementation (`app/vectorstore.py`)
- **Removed**: `faiss-cpu` dependency from `pyproject.toml`
- **Removed**: `VECTORSTORE_MODE` configuration option
- **Removed**: `INDEX_DIR` configuration option
- **Reason**: Simplify architecture and focus on ChromaDB as the production-grade solution
- **Benefits**:
  - Simpler codebase with less maintenance burden
  - No conditional logic for vector store selection
  - ChromaDB provides superior scalability and features (HNSW, persistence, metadata filtering)
  - Reduced dependencies and smaller installation footprint

#### Migration from FAISS to ChromaDB
If you were using FAISS (`VECTORSTORE_MODE=faiss`), you need to:
1. Remove `VECTORSTORE_MODE=faiss` from your `.env` file
2. Rebuild your index using ChromaDB: `python -m app.main --build`
3. Old FAISS indexes in `data/indexes/` are no longer used (can be safely deleted)

### ‚ö†Ô∏è Breaking Changes

**FAISS Support Removed**: The legacy FAISS vector store is no longer supported. ChromaDB is now the only vector store option. Users must rebuild their indexes with ChromaDB.

All other APIs remain backward compatible with v0.2.0.

### üêõ Bug Fixes
- Improved error handling in LLM invocations
- Better streaming support across all LLM providers
- More robust vectorstore initialization

### üìä Performance

No significant performance changes. LangChain adds minimal overhead while providing substantial benefits in composability and maintainability.

---

## [0.2.0] - 2026-01-31

### üöÄ Major New Features

#### Vector Store
- **ChromaDB Integration**: Added ChromaDB as the recommended vector store backend
  - HNSW indexing for sub-linear search performance
  - Native persistent storage (no manual save/load)
  - Efficient metadata filtering (pre-retrieval)
  - Incremental updates (no full rebuild required)
  - Scalable to millions of documents
  - New file: `app/vectorstore_chroma.py`
  - Configurable via `VECTORSTORE_MODE` (chroma/faiss)

#### Embeddings
- **E5 Multilingual**: Upgraded to `intfloat/multilingual-e5-large`
  - 1024-dimensional embeddings (vs 384 in v0.1)
  - Support for 100+ languages
  - Superior semantic search quality
  - Alternative models: e5-base (768d), e5-small (384d)
  - Configurable via `EMBED_MODEL`

#### Chunking
- **Semantic Chunking**: Intelligent content-aware text splitting
  - Analyzes sentence-level semantic similarity
  - Creates coherent chunks based on topic boundaries
  - Configurable similarity threshold
  - Falls back to sentence-aware splitting
  - New function: `chunk_text_semantic()` in `app/chunking.py`
  - Enable via `USE_SEMANTIC_CHUNKING=true`

#### Caching
- **Redis Integration**: Fast query result caching
  - Automatic caching of retrieval results
  - Configurable TTL (default: 1 hour)
  - Cache hit/miss tracking
  - Pattern-based cache clearing
  - New file: `app/cache.py`
  - Enable via `CACHE_ENABLED=true`

#### Monitoring
- **Prometheus Metrics**: Production-ready observability
  - Request latency histograms
  - Retrieval performance tracking
  - LLM token usage counters
  - Cache hit rate metrics
  - System health gauges
  - New file: `app/metrics.py`
  - Endpoints: `/metrics`, `/metrics/summary`
  - Enable via `METRICS_ENABLED=true`

#### Reranking
- **MMR (Maximal Marginal Relevance)**: Diversity-aware result selection
  - Balances relevance with diversity
  - Reduces redundancy in results
  - Configurable Œª parameter (0=diversity, 1=relevance)
  - New function: `maximal_marginal_relevance()` in `app/embeddings.py`
  - Enable via `RERANK_USE_MMR=true`

#### LLM Prompting
- **Chain-of-Thought**: Structured reasoning prompts
  - Step-by-step thinking framework
  - Few-shot examples included
  - Improved answer quality and citations
  - Updated `build_prompt()` in `app/rag.py`

### üìù Configuration Changes

#### New Settings
```env
# Vector Store
VECTORSTORE_MODE=chroma
CHROMA_DIR=data/chroma_db

# Embeddings
EMBED_MODEL=intfloat/multilingual-e5-large

# Chunking
USE_SEMANTIC_CHUNKING=true
SEMANTIC_SIMILARITY_THRESHOLD=0.7

# Reranking
RERANK_USE_MMR=true
MMR_LAMBDA=0.5

# Caching
CACHE_ENABLED=true
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=
CACHE_TTL=3600

# Monitoring
METRICS_ENABLED=true
PROMETHEUS_PORT=8001
```

### üîÑ API Changes

#### New Endpoints
- `GET /metrics` - Prometheus metrics in text format
- `GET /metrics/summary` - Human-readable metrics summary
- `GET /cache/stats` - Cache statistics and health
- `POST /cache/clear` - Clear cache by pattern

#### Updated Endpoints
- `POST /ask` - Now includes:
  - Cache support for faster responses
  - MMR reranking option
  - Detailed retrieval timing
  - Chain-of-thought prompting
- `GET /config` - Returns additional fields:
  - `vectorstore_mode`
  - `semantic_chunking`
  - `rerank_use_mmr`
  - `cache_enabled`
  - `metrics_enabled`

#### Response Schema Changes
- `AskResponse.retrieval_info` now includes:
  - `rerank_method`: "MMR", "Cross-Encoder", or null
  - `retrieval_time_ms`: Retrieval latency in milliseconds

### üì¶ Dependencies

#### Added
- `chromadb>=0.4.22` - Vector database
- `redis>=5.0.0` - Caching layer
- `prometheus-client>=0.19.0` - Metrics
- `spacy>=3.7.0` - NLP utilities

#### Updated
- Version bumped to `0.2.0` in `pyproject.toml`

### üèóÔ∏è Architecture Changes

#### New Files
- `app/vectorstore_chroma.py` - ChromaDB implementation
- `app/cache.py` - Redis caching layer
- `app/metrics.py` - Prometheus metrics

#### Modified Files
- `app/config.py` - Added new configuration options
- `app/chunking.py` - Added semantic chunking
- `app/embeddings.py` - Added MMR reranking
- `app/rag.py` - Improved prompt engineering
- `app/main.py` - Integrated all new features
- `.env.example` - Updated with new settings
- `README.md` - Comprehensive documentation update

### üéØ Performance Improvements

#### Latency Reductions
- **Cached queries**: ~50ms (vs 3-6s uncached)
- **ChromaDB search**: ~30ms on 10K docs (vs ~100ms FAISS)
- **MMR reranking**: ~100ms for 20‚Üí5 candidates

#### Scalability
- **ChromaDB**: Supports millions of documents (vs ~10K FAISS limit)
- **Disk-backed storage**: Lower memory footprint
- **Incremental updates**: No full rebuild required

### üêõ Bug Fixes
- Fixed import statement compatibility for newer Python versions
- Improved error handling in vector store operations
- Better fallback behavior when Redis is unavailable

### üìö Documentation

#### Updated
- `README.md` - Complete rewrite with:
  - Feature comparison tables (ChromaDB vs FAISS)
  - Performance benchmarks
  - Troubleshooting guide
  - Configuration examples
  - API endpoint documentation
- `.env.example` - Organized into sections with detailed comments

#### Added
- `CHANGELOG.md` - This file

### üîß Maintenance

#### Breaking Changes
‚ö†Ô∏è **Important**: The following changes may require configuration updates:

1. **Default Embedding Model**: Changed from `all-MiniLM-L6-v2` (384d) to `multilingual-e5-large` (1024d)
   - **Impact**: Existing FAISS indexes are incompatible
   - **Migration**: Rebuild index with `POST /build`

2. **Vector Store Mode**: New `VECTORSTORE_MODE` setting
   - **Default**: `chroma` (recommended)
   - **Legacy**: Set `VECTORSTORE_MODE=faiss` to use old behavior
   - **Impact**: Storage location changed from `data/indexes/` to `data/chroma_db/`

3. **Dependencies**: Redis required for caching
   - **Impact**: Caching features unavailable without Redis
   - **Workaround**: Set `CACHE_ENABLED=false` to disable

#### Deprecation Notices
- FAISS vector store is now legacy (still supported)
- Consider migrating to ChromaDB for better scalability

### üß™ Testing
- All existing tests pass with new features
- Additional test coverage needed for:
  - ChromaDB integration
  - MMR reranking
  - Cache layer

### üìä Metrics & Monitoring

#### Tracked Metrics
- **Requests**: Total count, error count, latency distribution
- **Retrieval**: Document count, score distribution, method latency
- **LLM**: Request count, token usage, generation latency
- **Cache**: Hit/miss rates, total keys
- **System**: Index size (documents, chunks)

#### Dashboards
- Prometheus metrics available at `:8001/metrics`
- Grafana dashboard template (coming soon)

---

## [0.1.0] - Initial Release

### Features
- Basic RAG pipeline with FAISS + BM25 hybrid search
- Sentence-aware chunking
- Cross-encoder reranking
- OpenRouter/Ollama LLM integration
- Web interface
- RAGAS evaluation

### Components
- FastAPI server
- FAISS vector store
- BM25 lexical search
- SentenceTransformers embeddings
- Streaming responses

---

**Migration Guide**: See [README.md](README.md#troubleshooting) for detailed upgrade instructions.
