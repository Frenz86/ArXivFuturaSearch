version: "3.9"

services:
  # =========================================================================
  # Main Application
  # =========================================================================
  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: arxiv-rag-copilot
    restart: unless-stopped
    ports:
      - "${APP_PORT:-8000}:8000"    # FastAPI application
      - "${PROMETHEUS_PORT:-8001}:8001"  # Prometheus metrics (optional)
    environment:
      # LLM Configuration
      - LLM_MODE=${LLM_MODE:-openrouter}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - OPENROUTER_MODEL=${OPENROUTER_MODEL:-anthropic/claude-3.5-sonnet}
      - OPENROUTER_TIMEOUT=${OPENROUTER_TIMEOUT:-120}
      - OPENROUTER_MAX_RETRIES=${OPENROUTER_MAX_RETRIES:-3}

      # Ollama (if using local LLM)
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.1}

      # Retrieval Configuration
      - TOP_K=${TOP_K:-5}
      - RETRIEVAL_K=${RETRIEVAL_K:-20}
      - CHUNK_SIZE=${CHUNK_SIZE:-900}
      - CHUNK_OVERLAP=${CHUNK_OVERLAP:-150}

      # Hybrid Search
      - SEMANTIC_WEIGHT=${SEMANTIC_WEIGHT:-0.7}
      - BM25_WEIGHT=${BM25_WEIGHT:-0.3}

      # Reranking
      - RERANK_ENABLED=${RERANK_ENABLED:-true}
      - RERANK_MODEL=${RERANK_MODEL:-cross-encoder/ms-marco-MiniLM-L-6-v2}
      - RERANK_USE_MMR=${RERANK_USE_MMR:-true}
      - MMR_LAMBDA=${MMR_LAMBDA:-0.5}

      # Embeddings
      - EMBED_MODEL=${EMBED_MODEL:-intfloat/multilingual-e5-large}

      # Semantic Chunking
      - USE_SEMANTIC_CHUNKING=${USE_SEMANTIC_CHUNKING:-true}
      - SEMANTIC_SIMILARITY_THRESHOLD=${SEMANTIC_SIMILARITY_THRESHOLD:-0.7}

      # Query Expansion
      - QUERY_EXPANSION_ENABLED=${QUERY_EXPANSION_ENABLED:-true}
      - QUERY_EXPANSION_METHOD=${QUERY_EXPANSION_METHOD:-acronym}

      # Redis Cache
      - CACHE_ENABLED=${CACHE_ENABLED:-true}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=${REDIS_DB:-0}
      - REDIS_PASSWORD=${REDIS_PASSWORD:-}
      - CACHE_TTL=${CACHE_TTL:-3600}

      # Monitoring
      - METRICS_ENABLED=${METRICS_ENABLED:-true}
      - PROMETHEUS_PORT=8001

      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_FORMAT=${LOG_FORMAT:-json}
    volumes:
      # Persist data directory (ChromaDB, raw data, processed data)
      - arxiv_data:/app/data
      # HuggingFace cache for models (embeddings, reranking)
      - hf_cache:/root/.cache/huggingface
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - arxiv_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # =========================================================================
  # Redis - Caching Layer
  # =========================================================================
  redis:
    image: redis:7-alpine
    container_name: arxiv-redis
    restart: unless-stopped
    command: >
      redis-server
      --maxmemory ${REDIS_MAX_MEMORY:-256mb}
      --maxmemory-policy allkeys-lru
      --save ${REDIS_SAVE:-""}
      --appendonly ${REDIS_APPENDONLY:-no}
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    networks:
      - arxiv_network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # =========================================================================
  # Ollama - Optional Local LLM Service
  # =========================================================================
  # Uncomment this service if you want to use Ollama for local LLM inference
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: arxiv-ollama
  #   restart: unless-stopped
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   environment:
  #     - OLLAMA_MODELS=${OLLAMA_MODEL:-llama3.1}
  #   networks:
  #     - arxiv_network
  #   # GPU support (uncomment if you have NVIDIA GPU with nvidia-docker)
  #   # deploy:
  #   #   resources:
  #   #     reservations:
  #   #       devices:
  #   #         - driver: nvidia
  #   #           count: 1
  #   #           capabilities: [gpu]

  # =========================================================================
  # Prometheus - Optional Metrics Collection
  # =========================================================================
  # Uncomment this service if you want to collect metrics
  # prometheus:
  #   image: prom/prometheus:latest
  #   container_name: arxiv-prometheus
  #   restart: unless-stopped
  #   ports:
  #     - "9090:9090"
  #   volumes:
  #     - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
  #     - prometheus_data:/prometheus
  #   command:
  #     - '--config.file=/etc/prometheus/prometheus.yml'
  #     - '--storage.tsdb.path=/prometheus'
  #   networks:
  #     - arxiv_network

  # =========================================================================
  # Grafana - Optional Metrics Visualization
  # =========================================================================
  # Uncomment this service if you want to visualize metrics
  # grafana:
  #   image: grafana/grafana:latest
  #   container_name: arxiv-grafana
  #   restart: unless-stopped
  #   ports:
  #     - "3000:3000"
  #   environment:
  #     - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
  #     - GF_USERS_ALLOW_SIGN_UP=false
  #   volumes:
  #     - grafana_data:/var/lib/grafana
  #   networks:
  #     - arxiv_network

# =============================================================================
# Networks
# =============================================================================
networks:
  arxiv_network:
    name: arxiv_network
    driver: bridge

# =============================================================================
# Volumes
# =============================================================================
volumes:
  # Application data (ChromaDB, downloaded papers, etc.)
  arxiv_data:
    name: arxiv_data
    driver: local

  # Redis persistence
  redis_data:
    name: arxiv_redis_data
    driver: local

  # HuggingFace model cache (persists downloaded models)
  hf_cache:
    name: arxiv_hf_cache
    driver: local

  # Ollama model storage (uncomment if using Ollama)
  # ollama_data:
  #   name: arxiv_ollama_data
  #   driver: local

  # Prometheus metrics storage (uncomment if using Prometheus)
  # prometheus_data:
  #   name: arxiv_prometheus_data
  #   driver: local

  # Grafana data storage (uncomment if using Grafana)
  # grafana_data:
  #   name: arxiv_grafana_data
  #   driver: local
